{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_1/Agentic_RAG/Upload_data_to_Qdrant_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq9fUQqW6fkx"
      },
      "source": [
        "# Uploading PDF Data to Qdrant with Embeddings\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to process unstructured document data (such as PDF files) and store it in a local vector database using Qdrant. This workflow is useful for building applications like intelligent document search, semantic search engines, or AI-based question-answering systems.\n",
        "\n",
        "We will start by extracting text content from PDF files, convert that text into numerical representations called embeddings, and finally upload those embeddings into a Qdrant database for efficient retrieval and future use.\n",
        "\n",
        "In this example, we will use two types of PDF data:\n",
        "- **OpenAI documentation** (covering tools, APIs, and usage guidelines).\n",
        "- **10-K financial filings** (official company reports and financial statements by Uber and Lyft).\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- **Extract text from PDF files** using the PyMuPDF library.\n",
        "- **Generate semantic embeddings** for each chunk of text using a model from Nomic from Hugging Face.\n",
        "- **Store the embeddings in Qdrant**, a vector database running locally.\n",
        "\n",
        "By the end of this notebook, you'll have a working pipeline that reads documents, encodes them into meaningful vector representations, and persists them in a local database that can be queried later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZLVukPv8LpW"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "Before we begin, ensure the necessary libraries are installed and imported\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HciDI6OpSKJN"
      },
      "outputs": [],
      "source": [
        "# Install the Qdrant client, which allows you to connect to and interact with a Qdrant vector database.\n",
        "# Qdrant is often used for similarity search like semantic search or recommendation systems.\n",
        "!pip install qdrant_client\n",
        "\n",
        "# Install the Hugging Face Transformers library.\n",
        "# This library provides pre-trained models for tasks like text embeddings, classification, translation, summarization, and more.\n",
        "!pip install transformers\n",
        "\n",
        "# Install the PyMuPDF library (also known as Fitz), which is used for working with PDF files.\n",
        "# It allows you to extract text, images, and metadata from PDFs,\n",
        "!pip install PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# Import the PyMuPDF library, which is installed as 'fitz'\n",
        "# This library is used for reading and extracting content from PDF documents.\n",
        "import fitz\n",
        "\n",
        "# Import the 'os' module for handling file paths and operating system interactions (not used directly here but often useful)\n",
        "import os"
      ],
      "metadata": {
        "id": "mKrnG6z5REv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swim7SqVW7iC"
      },
      "source": [
        "## 1. Extract Data from PDF Files\n",
        "\n",
        "In this step, we will use the PyMuPDF library to extract text from our PDF documents. This will allow us to process the content and prepare it for embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaQO3vGva71L"
      },
      "outputs": [],
      "source": [
        "def read_text_pymupdf(path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file using the PyMuPDF (fitz) library.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing all the text extracted from the PDF.\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the PDF document using the provided file path\n",
        "    # This returns a Document object that allows access to each page\n",
        "    doc = fitz.open(path)\n",
        "\n",
        "    # Initialize an empty string to collect text from all pages\n",
        "    text_results = ''\n",
        "\n",
        "    # Loop through each page in the PDF document\n",
        "    for page in doc:\n",
        "        # Extract text content from the current page\n",
        "        text = page.get_text()\n",
        "\n",
        "        # Append the extracted text to the cumulative result\n",
        "        text_results += text\n",
        "\n",
        "    # Return the complete text from the PDF\n",
        "    return text_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf0_uV6kbDpZ"
      },
      "outputs": [],
      "source": [
        "# Define the path to the folder containing OpenAI-related PDF documents\n",
        "document_path_opnai = \"/content/drive/MyDrive/Router RAG docs/openai\" #make sure to add your folder path here, this data is available in the github repository for the course\n",
        "\n",
        "# Define the path to the folder containing 10-K filing PDF documents\n",
        "document_path_10k = \"/content/drive/MyDrive/Router RAG docs/10k files\" #make sure to add your folder path here, this data is available in the github repository for the course\n",
        "\n",
        "# Initialize an empty list to hold the extracted text from the OpenAI documents\n",
        "openai_docs = []\n",
        "\n",
        "# Initialize an empty list to hold the extracted text from the 10-K documents\n",
        "docs_10k = []\n",
        "\n",
        "# Sets to track processed filenames (avoid duplicates)\n",
        "seen_files_opnai = set()\n",
        "seen_files_10k = set()\n",
        "\n",
        "# Loop through each file in the OpenAI documents folder\n",
        "for _f in os.listdir(document_path_opnai):\n",
        "    # Process only PDF files and skip duplicate filenames\n",
        "    if _f.lower().endswith(\".pdf\") and _f not in seen_files_opnai:\n",
        "        path = os.path.join(document_path_opnai, _f)\n",
        "        openai_docs.append(read_text_pymupdf(path))\n",
        "        seen_files_opnai.add(_f)\n",
        "\n",
        "# Loop through each file in the 10-K documents folder\n",
        "for _f in os.listdir(document_path_10k):\n",
        "    # Process only PDF files and skip duplicate filenames\n",
        "    if _f.lower().endswith(\".pdf\") and _f not in seen_files_10k:\n",
        "        path = os.path.join(document_path_10k, _f)\n",
        "        docs_10k.append(read_text_pymupdf(path))\n",
        "        seen_files_10k.add(_f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VXD8vvHQdIKU"
      },
      "outputs": [],
      "source": [
        "docs_10k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-3b8pckDzY6"
      },
      "source": [
        "## 2. Prepare Document Chunks with Metadata for Vector Storage\n",
        "\n",
        "Before we store our documents in a vector database like Qdrant, it's important to organize the data in a meaningful way. This section assigns unique identifiers and metadata to each document chunk.\n",
        "\n",
        "### Why this step is important:\n",
        "\n",
        "- **Chunk-Level Tracking**: Each document is split into smaller text chunks to fit the input size of embedding models. Assigning metadata to each chunk helps trace it back to its original source.\n",
        "  \n",
        "- **UUID Generation**: By attaching a universally unique identifier (UUID) to each chunk, we ensure every piece of data can be reliably referenced or retrieved later.\n",
        "\n",
        "- **Metadata Enrichment**: Adding metadata such as the original document path enables better filtering, searching, and organization within the vector database.\n",
        "\n",
        "This process ensures that once the chunks are embedded and stored, they remain well-organized and easily searchable in downstream applications such as semantic search or document-based Q&A systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7yqrqkXLdSCG"
      },
      "outputs": [],
      "source": [
        "# Import a text splitter that breaks large texts into smaller, overlapping chunks\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create the text splitter with settings for chunk size and overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2048,          # Max characters per chunk\n",
        "    chunk_overlap=50,         # Overlap between chunks to preserve context\n",
        "    length_function=len,      # Use Python's len() to measure text length\n",
        "    is_separator_regex=False, # Treat separators as plain text, not regex\n",
        "    separators=[              # Preferred breakpoints for splitting\n",
        "        \"\\n\\n\", \"\\n\", \" \", \".\", \",\",\n",
        "        \"\\u200b\", \"\\uff0c\", \"\\u3001\", \"\\uff0e\", \"\\u3002\", \"\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Split the extracted OpenAI document text into chunks\n",
        "opnai_chunks = text_splitter.create_documents(openai_docs)\n",
        "\n",
        "# Split the 10-K documents into chunks as well\n",
        "chunks_10k = text_splitter.create_documents(docs_10k)\n",
        "\n",
        "# View the result (list of text chunks)\n",
        "opnai_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haph5n9eihN3"
      },
      "outputs": [],
      "source": [
        "# Import the uuid module to generate unique identifiers\n",
        "import uuid\n",
        "\n",
        "# Loop through each chunk of the OpenAI documents\n",
        "for i in range(len(opnai_chunks)):\n",
        "    # Generate a unique ID for each chunk (helps track and reference later)\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # Add metadata to the chunk:\n",
        "    # 'document_info' stores the source path (where the document came from)\n",
        "    # 'uuid' stores a unique identifier for the chunk\n",
        "    opnai_chunks[i].metadata['document_info'] = document_path_opnai\n",
        "    opnai_chunks[i].metadata['uuid'] = unique_id\n",
        "\n",
        "# Display the first 5 chunks with their metadata\n",
        "opnai_chunks[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "brsB1FhufrCp"
      },
      "outputs": [],
      "source": [
        "# Loop through each chunk of the 10-K documents\n",
        "for i in range(len(chunks_10k)):\n",
        "    # Generate a unique identifier (UUID) for each chunk\n",
        "    unique_id = str(uuid.uuid4())\n",
        "\n",
        "    # Assign metadata to each chunk:\n",
        "    # 'document_info' stores the path of the folder where the 10-K files are located\n",
        "    # 'uuid' stores the unique identifier for the chunk\n",
        "    chunks_10k[i].metadata['document_info'] = document_path_10k\n",
        "    chunks_10k[i].metadata['uuid'] = unique_id\n",
        "\n",
        "# Display the metadata and content of the 6th chunk (index 5) from the 10-K documents\n",
        "chunks_10k[5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCKR7-IzEqW2"
      },
      "source": [
        "## 3. Embed Chunks for Vector Database\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Embedding the chunks of text is a crucial step in preparing the data for storage in a **vector database** like Qdrant. In this step, we convert each text chunk into a **numerical representation** (vector) that captures its semantic meaning. This allows us to perform advanced operations like **semantic search**, **similarity comparison**, and **retrieval** based on meaning, rather than just keyword matching.\n",
        "\n",
        "### Why we Embed?\n",
        "\n",
        "- **Vector Representation**: Embedding transforms text into vectors (lists of numbers) that machine learning models can understand. These vectors represent the **semantic meaning** of the text, allowing similar texts to be grouped together, even if they don’t share exact words.\n",
        "  \n",
        "- **Efficient Search**: Storing these vectors in a vector database enables **fast similarity searches**. For example, if you ask a question or search for a document, the vector database can quickly find and return the most relevant results based on the meaning of the text, not just exact matches.\n",
        "  \n",
        "- **Contextual Understanding**: The embedding process allows the system to \"understand\" the context of words, which improves the relevance and accuracy of search results. For example, it helps understand that \"machine learning\" and \"artificial intelligence\" are related concepts, even if they don't appear together in the same document.\n",
        "\n",
        "### What it Does?\n",
        "\n",
        "- **Transforms text** into dense vectors, capturing the **semantic essence** of the content.\n",
        "- These vectors are then stored in a **vector database** (Qdrant in our case) for fast retrieval and comparison based on similarity, enabling features like document search or answering questions.\n",
        "  \n",
        "In the following steps, we will embed the text chunks using a pre-trained nomic text embed model, and store the resulting vectors in Qdrant for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIA6DUb3hG3I"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries from the Hugging Face Transformers library\n",
        "# AutoTokenizer is used to tokenize the input text into a format that the model can process.\n",
        "# AutoModel loads the pre-trained model for generating embeddings.\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load the tokenizer and model from Hugging Face\n",
        "# 'nomic-ai/nomic-embed-text-v1.5' is a pre-trained model designed for text embeddings\n",
        "# The `trust_remote_code=True` argument allows the use of the model's code from the remote repository\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
        "text_model = AutoModel.from_pretrained(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
        "\n",
        "# Define a function to get embeddings for a given text input\n",
        "def get_text_embeddings(text):\n",
        "    # Tokenize the text input into format the model understands\n",
        "    # `return_tensors=\"pt\"` means the output will be in PyTorch tensor format\n",
        "    # `padding=True` ensures that inputs of varying lengths are padded to a consistent length\n",
        "    # `truncation=True` ensures that long inputs are truncated to fit the model's max input length\n",
        "    inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Pass the tokenized inputs into the model to get embeddings\n",
        "    # The model's output contains hidden states from the transformer layers\n",
        "    outputs = text_model(**inputs)\n",
        "\n",
        "    # We use the mean of the last hidden state (output of the final layer) for each token\n",
        "    # This gives us a fixed-size vector for the input text\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Return the embeddings as a numpy array (for use in further processing or storage)\n",
        "    return embeddings[0].detach().numpy()  # Detach the tensor from the computation graph and convert to numpy\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a test sentence.\"  # Sample text to embed\n",
        "embeddings = get_text_embeddings(text)  # Get the embeddings for the text\n",
        "\n",
        "# Print the first 5 values of the embeddings (just to inspect the result)\n",
        "print(embeddings[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ms4hviWkf37"
      },
      "outputs": [],
      "source": [
        "# Embed the OpenAI document chunks into vectors using the `get_text_embeddings` function\n",
        "# For each chunk in the OpenAI documents, the text is passed through the embedding model\n",
        "# `document.page_content` refers to the actual text content of each chunk.\n",
        "opnai_texts_embeded = [get_text_embeddings(document.page_content) for document in opnai_chunks]\n",
        "\n",
        "# Embed the 10-K document chunks into vectors in the same way\n",
        "# For each chunk in the 10-K documents, the `page_content` is passed to the embedding model\n",
        "texts_embeded_10k = [get_text_embeddings(document.page_content) for document in chunks_10k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrkE-V4PlClH"
      },
      "source": [
        "##  4. Initialize Qdrant\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "This step initializes **Qdrant**, a vector database, to store the text embeddings generated earlier. Qdrant is designed for high-performance similarity search and retrieval of vector data, making it ideal for tasks like semantic search, recommendation systems, or nearest neighbor searches.\n",
        "\n",
        "### Why this is important?\n",
        "\n",
        "- **Qdrant Initialization**: This code sets up the Qdrant database to store vectors efficiently.\n",
        "- **Creating a Collection**: Collections are like tables in a relational database, and in this case, the collection stores vectors representing text embeddings.\n",
        "- **Cosine Similarity**: Using cosine similarity enables the database to quickly identify vectors (texts) that are semantically similar to a given query.\n",
        "  \n",
        "Once the collection is created and initialized, it will be ready to store the embeddings from the OpenAI documents and 10-K filings, making it possible to perform fast similarity searches later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpMMBShOk-Fd"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries from the Qdrant client\n",
        "# QdrantClient is used to interact with the Qdrant database.\n",
        "# models provides predefined parameters like vector configurations.\n",
        "from qdrant_client import QdrantClient, models\n",
        "import os\n",
        "\n",
        "# Define the path where the Qdrant database will be stored\n",
        "# This is the directory where Qdrant will save its data on your local machine\n",
        "qdrant_data_dir = '/content/qdrant_data'\n",
        "\n",
        "# Create the directory if it doesn't already exist\n",
        "# `exist_ok=True` ensures no error is raised if the directory already exists\n",
        "os.makedirs(qdrant_data_dir, exist_ok=True)\n",
        "\n",
        "# Initialize the Qdrant client, passing the path where the database will be stored\n",
        "# The Qdrant client will manage operations like adding vectors, creating collections, and querying data\n",
        "client = QdrantClient(path=qdrant_data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAo5IyCElIbj"
      },
      "outputs": [],
      "source": [
        "# Determine the size (dimensionality) of the embeddings by checking the length of the first embedding vector\n",
        "# The embedding size is the number of values in each vector representation of the text (e.g., 768, 1024, etc.)\n",
        "text_embeddings_size = len(opnai_texts_embeded[0])\n",
        "\n",
        "# Check if the collection \"opnai_data\" already exists in Qdrant\n",
        "# If it does not exist, the code proceeds to create a new collection\n",
        "if not client.collection_exists(\"opnai_data\"):\n",
        "    # Create a new collection in Qdrant to store the OpenAI document embeddings\n",
        "    client.create_collection(\n",
        "        # Name of the collection, which helps identify it in the database\n",
        "        collection_name=\"opnai_data\",\n",
        "\n",
        "        # Define the vector configuration (size and distance metric) for the collection\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=text_embeddings_size,  # Set the vector size to match the embeddings' dimensionality\n",
        "            distance=models.Distance.COSINE,  # Use Cosine similarity to measure distance between vectors\n",
        "        ),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29OrYi5ulNQx"
      },
      "outputs": [],
      "source": [
        "# Check if the collection \"10k_data\" already exists in Qdrant\n",
        "# If it does not exist, the code proceeds to create a new collection\n",
        "if not client.collection_exists(\"10k_data\"):\n",
        "    # Create a new collection in Qdrant to store the 10-K document embeddings\n",
        "    client.create_collection(\n",
        "        # Name of the collection, which will hold the 10-K document embeddings\n",
        "        collection_name=\"10k_data\",\n",
        "\n",
        "        # Define the vector configuration (size and distance metric) for the collection\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=text_embeddings_size,  # Set the vector size to match the embeddings' dimensionality\n",
        "            distance=models.Distance.COSINE,  # Use Cosine similarity to measure distance between vectors\n",
        "        ),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuuPLjUcltrQ"
      },
      "source": [
        "## 5. Store Embeddings in Qdrant\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "In this step, we insert the embeddings (numerical representations of text) for both the OpenAI documents and the 10-K filings into their respective collections in Qdrant. By storing these embeddings in Qdrant, we can enable efficient semantic search functionality, which is a key part of the **Agentic RAG** (Retriever-Augmented Generation) project.\n",
        "\n",
        "Once the embeddings are stored, we can leverage Qdrant's similarity search capabilities to retrieve semantically relevant documents based on a given query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjax3ybnlmcJ"
      },
      "outputs": [],
      "source": [
        "# Define the names of the Qdrant collections we're working with\n",
        "# These are the collections where document embeddings will be stored\n",
        "clusters = [\"opnai_data\", \"10k_data\"]\n",
        "\n",
        "# Import numpy to handle vector data as arrays\n",
        "import numpy as np\n",
        "\n",
        "# Upload (store) the embeddings of OpenAI documents into the 'opnai_data' collection in Qdrant\n",
        "client.upload_points(\n",
        "    collection_name=\"opnai_data\",  # Name of the target collection in Qdrant\n",
        "\n",
        "    # Create a list of PointStruct objects, one for each embedded document chunk\n",
        "    points=[\n",
        "        models.PointStruct(\n",
        "            id=doc.metadata['uuid'],  # Use the pre-generated UUID as a unique ID for each point (chunk)\n",
        "\n",
        "            vector=np.array(opnai_texts_embeded[idx]),  # The embedding vector for the chunk, converted to a NumPy array\n",
        "\n",
        "            payload={  # Payload stores extra information (metadata and content) along with the vector\n",
        "                \"metadata\": doc.metadata,       # Include metadata like document path and UUID\n",
        "                \"content\": doc.page_content     # Store the original text content of the chunk\n",
        "            }\n",
        "        )\n",
        "        for idx, doc in enumerate(opnai_chunks)  # Loop through all OpenAI chunks and embed them\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmRcz7pdmd9N"
      },
      "outputs": [],
      "source": [
        "# Upload (store) the embeddings of 10-K documents into the '10k_data' collection in Qdrant\n",
        "client.upload_points(\n",
        "    collection_name=\"10k_data\",  # Target collection name in Qdrant\n",
        "\n",
        "    # Create a list of points, one for each chunk in the 10-K documents\n",
        "    points=[\n",
        "        models.PointStruct(\n",
        "            id=doc.metadata['uuid'],  # Unique ID for each vector (from previously assigned UUID)\n",
        "\n",
        "            vector=np.array(texts_embeded_10k[idx]),  # The embedding vector for the chunk, converted to NumPy array\n",
        "\n",
        "            payload={  # Payload contains additional information for each vector\n",
        "                \"metadata\": doc.metadata,       # Include metadata such as source path and UUID\n",
        "                \"content\": doc.page_content     # Include the original chunk text for retrieval/display\n",
        "            }\n",
        "        )\n",
        "        for idx, doc in enumerate(chunks_10k)  # Loop through all 10-K chunks to create PointStructs\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp1r2GnnJ1rv"
      },
      "source": [
        "## Final Notes\n",
        "\n",
        "At this point, we've successfully completed the full pipeline for preparing and storing document embeddings:\n",
        "\n",
        "1. Extracted text from PDFs using PyMuPDF.\n",
        "2. Split the documents into manageable text chunks.\n",
        "3. Embedded each chunk using the `nomic-embed-text` model.\n",
        "4. Stored the resulting vectors, along with metadata and original content, into Qdrant collections (`opnai_data` and `10k_data`).\n",
        "\n",
        "These embeddings are now stored in a local Qdrant vector database located at:\n",
        "\n",
        "/content/qdrant_data\n",
        "\n",
        "\n",
        "You can reuse this vector database in any other project by pointing to the same path. In our case, we will be using it as the retrieval layer in the **Agentic RAG** system, where relevant chunks will be fetched based on user queries and passed to an LLM to generate meaningful, grounded responses.\n",
        "\n",
        "This setup forms the foundation for building retrieval-augmented applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSfLvsFAKce3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}