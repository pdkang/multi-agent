{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzafarooq/multi-agent-course/blob/main/Module_2/Quantization/TextStreamer_Meta_Llama_3_1_8B_Instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Text Streaming with Hugging Face Transformers and Llama 3.1 8B\n",
        "\n",
        "This notebook demonstrates how to use the `TextStreamer` and `TextIteratorStreamer` utilities from the Hugging Face `transformers` library for generating text token by token with the `unsloth/Meta-Llama-3.1-8B-Instruct` model. Streaming is crucial for applications requiring real-time output, such as chatbots, as it allows users to see the response as it's being generated, rather than waiting for the entire sequence.\n",
        "\n",
        "We will explore two main scenarios:\n",
        "1.  **Text Streaming without Quantization:** Demonstrates basic streaming with the model in its default precision.\n",
        "2.  **Text Streaming with Quantization (4-bit):** Shows how to stream text while using a memory-efficient quantized version of the model.\n",
        "\n",
        "Additionally, this notebook includes helper functions to monitor GPU memory usage and provides an example of how to calculate common Large Language Model (LLM) performance metrics suchs as:\n",
        "- Time To First Token (TTFT)\n",
        "- Inter-Token Latency (ITL)\n",
        "- End-to-end Latency\n",
        "- Throughput"
      ],
      "metadata": {
        "id": "rZjNoO75YOFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "This section imports the necessary libraries for model loading, tokenization, text streaming, and GPU monitoring."
      ],
      "metadata": {
        "id": "kOZKt5GKYdRc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5szSg-xijVEM"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "# %%capture will suppress the output of this cell\n",
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBbu71zgnm4d"
      },
      "outputs": [],
      "source": [
        "# Import core libraries from Hugging Face and PyTorch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU Utility Functions\n",
        "\n",
        "The following functions are defined to help monitor GPU memory usage before and after loading the model. This is useful for understanding the memory footprint of different model configurations (e.g., full precision vs. quantized)."
      ],
      "metadata": {
        "id": "zDgHGMHheWRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwn0ucrtnm4d"
      },
      "outputs": [],
      "source": [
        "# Define helper functions to display GPU memory statistics\n",
        "# This helps in understanding the memory footprint of the models.\n",
        "\n",
        "def start_gpu_stat():\n",
        "    \"\"\"Records and prints initial GPU memory stats before model loading.\n",
        "\n",
        "    This function captures the currently reserved GPU memory and the total\n",
        "    available GPU memory, printing these values in GB. It's intended to be\n",
        "    called before a memory-intensive operation like loading a large model\n",
        "    to establish a baseline.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - initial_gpu_memory (float): The initially reserved GPU memory in GB.\n",
        "            - max_memory (float): The total available GPU memory in GB.\n",
        "    \"\"\"\n",
        "    #@title Show current memory stats\n",
        "    # torch.cuda.get_device_properties(0) gets properties of the default CUDA device.\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    # torch.cuda.max_memory_reserved() returns the maximum GPU memory managed by the caching allocator in bytes.\n",
        "    initial_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) # Convert to GB\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) # Convert to GB\n",
        "    print(f\"Initial Max memory reserved: {initial_gpu_memory} GB / Max memory available: {max_memory} GB\")\n",
        "    return initial_gpu_memory, max_memory\n",
        "\n",
        "\n",
        "def final_gpu_stat(_initial_gpu_memory, _max_memory):\n",
        "    \"\"\"Calculates and prints GPU memory usage after an operation, like model loading.\n",
        "\n",
        "    This function determines the peak reserved GPU memory after an operation,\n",
        "    the difference in memory usage from the initial state, and presents these\n",
        "    as absolute values (GB) and percentages of total available memory.\n",
        "\n",
        "    Args:\n",
        "        _initial_gpu_memory (float): The initially reserved GPU memory in GB,\n",
        "                                     as returned by start_gpu_stat().\n",
        "        _max_memory (float): The total available GPU memory in GB,\n",
        "                             as returned by start_gpu_stat().\n",
        "    \"\"\"\n",
        "    #@title Show final memory and time stats\n",
        "    # Calculates and prints the peak GPU memory usage and the difference from the initial state.\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    used_memory_for_diff = round(used_memory - _initial_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory         /_max_memory*100, 3)\n",
        "    diff_percentage = round(used_memory_for_diff/_max_memory*100, 3)\n",
        "\n",
        "    print(f\"Max memory = {_max_memory} GB.\")\n",
        "    print(f\"{_initial_gpu_memory} GB of INITIAL memory reserved.\")\n",
        "    print(f\"Peak reserved FINAL memory = {used_memory} GB.\")\n",
        "    print(f\"Peak reserved memory DIFFERENCE = {used_memory_for_diff} GB.\")\n",
        "    print(f\"Peak reserved memory % of FINAL memory = {used_percentage} %.\")\n",
        "    print(f\"Peak reserved memory % of DIFFERENCE memory = {diff_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wELsOde0nm4d"
      },
      "source": [
        "# 1. Text Streaming Without Quantization\n",
        "\n",
        "This section demonstrates text streaming using the `unsloth/Meta-Llama-3.1-8B-Instruct` model loaded in its default precision (typically float16 or float32). This provides the highest accuracy but consumes more memory and may be slower compared to quantized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHuonjNTiKPb"
      },
      "outputs": [],
      "source": [
        "# Define the model ID for Hugging Face Hub\n",
        "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct\" # Replace with your model\n",
        "\n",
        "# Load tokenizer and full precision model\n",
        "# The tokenizer converts text into a format (tokens) that the model can understand.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Record GPU memory before loading the model\n",
        "initial_gpu_memory, max_memory = start_gpu_stat()\n",
        "\n",
        "# Load the Causal Language Model\n",
        "# AutoModelForCausalLM is used for models that predict the next token in a sequence (e.g., GPT-like models).\n",
        "# device_map=\"auto\" automatically distributes the model across available GPUs/CPU based on resources.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Record GPU memory after loading the model\n",
        "final_gpu_stat(initial_gpu_memory, max_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Using `TextStreamer`\n",
        "\n",
        "The `TextStreamer` class provides a simple way to print the generated tokens to the console as they are produced. It's useful for immediate visual feedback during interactive sessions."
      ],
      "metadata": {
        "id": "UwZW2aL9fUTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sZ5lDoL4yQ6"
      },
      "outputs": [],
      "source": [
        "# Prepare the prompt using an Alpaca-style format\n",
        "# This common instruction-following format helps the model understand the task.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")  # instruction\n",
        "\n",
        "# Tokenize the input prompt and move it to the model's device (e.g., GPU)\n",
        "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)  # Move inputs to model's device\n",
        "\n",
        "# Initialize text streamer\n",
        "# skip_prompt=False: The input prompt will also be printed by the streamer.\n",
        "# skip_special_tokens=False: Special tokens (like EOS, BOS) will be printed.\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=False)\n",
        "\n",
        "# Generate response with streamer\n",
        "# The model.generate() method will call the streamer for each new token.\n",
        "# max_new_tokens limits the length of the generated response.\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Understanding Performance Metrics: TTFT, ITL, Latency, Throughput\n",
        "\n",
        "When evaluating streaming performance, several metrics are important:\n",
        "\n",
        "-   **Time To First Token (TTFT):** The time elapsed from sending the request until the first token of the response is received. A lower TTFT means a quicker initial response.\n",
        "-   **Inter-Token Latency (ITL):** The average time taken to generate each subsequent token after the first one. Lower ITL means faster streaming of the rest ofthe response.\n",
        "-   **End-to-end Latency:** The total time taken from sending the request to receiving the complete response.\n",
        "    -   *Approximation: Average output length (in tokens) * Inter-token latency + TTFT*\n",
        "-   **Throughput:** The number of output tokens generated per second. Higher throughput indicates better overall generation speed."
      ],
      "metadata": {
        "id": "E7u2BzNmf1uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Using `TextIteratorStreamer` and Calculating Performance Metrics\n",
        "\n",
        "The `TextIteratorStreamer` allows for more programmatic control over the streamed tokens. It makes the generation process iterable, so you can process each token (or chunk of tokens) as it's generated. This is useful for applications where you need to handle the output in a custom way (e.g., sending it over a network, updating a UI).\n",
        "\n",
        "This section also demonstrates how to calculate TTFT, ITL, end-to-end latency, and throughput."
      ],
      "metadata": {
        "id": "HGwBtzLpf7Rx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoOe8cFznm4e"
      },
      "outputs": [],
      "source": [
        "# Prepare the prompt using an Alpaca-style format\n",
        "# This common instruction-following format helps the model understand the task.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")\n",
        "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Initialize variables for time measurements\n",
        "start_time = time.time()\n",
        "token_times = []\n",
        "\n",
        "# Initialize TextIteratorStreamer\n",
        "# skip_prompt=True: The input prompt will not be part of the streamed output.\n",
        "# skip_special_tokens=False: Special tokens will be included in the stream.\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Start generation in a separate thread\n",
        "# This is crucial because model.generate() with a streamer is a blocking call.\n",
        "# Running it in a thread allows the main thread to iterate over the streamer simultaneously.\n",
        "thread = Thread(target=model.generate, kwargs={\n",
        "    'input_ids': inputs['input_ids'],\n",
        "    'attention_mask': inputs['attention_mask'],\n",
        "    'streamer': streamer,\n",
        "    'max_new_tokens': 100\n",
        "})\n",
        "thread.start()\n",
        "\n",
        "# Initialize a variable to store the model output\n",
        "model_output = \"\"\n",
        "first_token_time = None\n",
        "\n",
        "# Iterate over the streamer to get generated text token by token (or small chunks)\n",
        "for i, new_text in enumerate(streamer):\n",
        "    model_output += new_text\n",
        "    print(new_text, end='')\n",
        "\n",
        "    # Measure time for the first token\n",
        "    if i == 0:\n",
        "        first_token_time = time.time()\n",
        "    # Measure time for each token\n",
        "    token_times.append(time.time())\n",
        "\n",
        "# Calculate end-to-end latency\n",
        "end_time = time.time()\n",
        "end_to_end_latency = end_time - start_time\n",
        "\n",
        "# Calculate Time To First Token (TTFT)\n",
        "ttft = first_token_time - start_time if first_token_time else 0\n",
        "\n",
        "# Calculate Inter-Token Latency (ITL)\n",
        "itl = sum(x - y for x, y in zip(token_times[1:], token_times[:-1])) / (len(token_times) - 1) if len(token_times) > 1 else 0\n",
        "\n",
        "# Calculate Throughput (tokens per second)\n",
        "# Note: tokenizer.encode(model_output) re-tokenizes the output string.\n",
        "# For more precise token count, you could count tokens as they arrive if the streamer yields individual tokens.\n",
        "throughput = len(tokenizer.encode(model_output)) / end_to_end_latency if model_output else 0\n",
        "\n",
        "print(\"\\nTime To First Token (TTFT):\", ttft)\n",
        "print(\"Inter-token latency (ITL):\", itl)\n",
        "print(\"End-to-end Latency:\", end_to_end_latency)\n",
        "print(\"Throughput:\", throughput)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTHR-8MvvrY"
      },
      "source": [
        "# 2. Text Streaming With Quantization (4-bit)\n",
        "\n",
        "Quantization is a technique to reduce the memory footprint and potentially speed up the inference of large language models. Here, we use 4-bit quantization (`BitsAndBytesConfig`) to load the model. This significantly reduces the GPU VRAM required, making it possible to run larger models on consumer hardware. We will repeat the streaming exercises with this quantized model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important:** Shutdown and restart the kernel before running the cells below if you have already run the non-quantized model. This ensures that GPU memory is cleared, providing accurate memory usage statistics for the quantized model."
      ],
      "metadata": {
        "id": "dhqveNHiid_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Setup for Quantized Model\n",
        "\n",
        "Re-import libraries and redefine GPU utility functions if the kernel was restarted. Then, configure and load the model with 4-bit quantization."
      ],
      "metadata": {
        "id": "F3Bfas65iSKC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPa8CI-mnm4f"
      },
      "outputs": [],
      "source": [
        "# Re-import necessary libraries (if kernel was restarted)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlXRyTWhnm4f"
      },
      "outputs": [],
      "source": [
        "# Define helper functions to display GPU memory statistics\n",
        "# This helps in understanding the memory footprint of the models.\n",
        "\n",
        "\n",
        "def start_gpu_stat():\n",
        "    \"\"\"Records and prints initial GPU memory stats before model loading.\n",
        "\n",
        "    This function captures the currently reserved GPU memory and the total\n",
        "    available GPU memory, printing these values in GB. It's intended to be\n",
        "    called before a memory-intensive operation like loading a large model\n",
        "    to establish a baseline.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - initial_gpu_memory (float): The initially reserved GPU memory in GB.\n",
        "            - max_memory (float): The total available GPU memory in GB.\n",
        "    \"\"\"\n",
        "    #@title Show current memory stats\n",
        "    # torch.cuda.get_device_properties(0) gets properties of the default CUDA device.\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    # torch.cuda.max_memory_reserved() returns the maximum GPU memory managed by the caching allocator in bytes.\n",
        "    initial_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) # Convert to GB\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) # Convert to GB\n",
        "    print(f\"Initial Max memory reserved: {initial_gpu_memory} GB / Max memory available: {max_memory} GB\")\n",
        "    return initial_gpu_memory, max_memory\n",
        "\n",
        "\n",
        "def final_gpu_stat(_initial_gpu_memory, _max_memory):\n",
        "    \"\"\"Calculates and prints GPU memory usage after an operation, like model loading.\n",
        "\n",
        "    This function determines the peak reserved GPU memory after an operation,\n",
        "    the difference in memory usage from the initial state, and presents these\n",
        "    as absolute values (GB) and percentages of total available memory.\n",
        "\n",
        "    Args:\n",
        "        _initial_gpu_memory (float): The initially reserved GPU memory in GB,\n",
        "                                     as returned by start_gpu_stat().\n",
        "        _max_memory (float): The total available GPU memory in GB,\n",
        "                             as returned by start_gpu_stat().\n",
        "    \"\"\"\n",
        "    #@title Show final memory and time stats\n",
        "    # Calculates and prints the peak GPU memory usage and the difference from the initial state.\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    used_memory_for_diff = round(used_memory - _initial_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory         /_max_memory*100, 3)\n",
        "    diff_percentage = round(used_memory_for_diff/_max_memory*100, 3)\n",
        "\n",
        "    print(f\"Max memory = {_max_memory} GB.\")\n",
        "    print(f\"{_initial_gpu_memory} GB of INITIAL memory reserved.\")\n",
        "    print(f\"Peak reserved FINAL memory = {used_memory} GB.\")\n",
        "    print(f\"Peak reserved memory DIFFERENCE = {used_memory_for_diff} GB.\")\n",
        "    print(f\"Peak reserved memory % of FINAL memory = {used_percentage} %.\")\n",
        "    print(f\"Peak reserved memory % of DIFFERENCE memory = {diff_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V4lZeLtnm4f"
      },
      "outputs": [],
      "source": [
        "# Define the model ID\n",
        "model_id = \"unsloth/Meta-Llama-3.1-8B-Instruct\" # Replace with your model\n",
        "\n",
        "# Configure 4-bit quantization using BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load tokenizer and model in 4-bit\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Record GPU memory before loading the quantized model\n",
        "initial_gpu_memory, max_memory = start_gpu_stat()\n",
        "\n",
        "# Load the model with quantization configuration\n",
        "# device_map=\"auto\" will handle placing the quantized model layers.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Record GPU memory after loading the quantized model\n",
        "final_gpu_stat(initial_gpu_memory, max_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Using `TextStreamer` with Quantized Model\n",
        "\n",
        "Now, we use `TextStreamer` with the 4-bit quantized model. The process is identical to the non-quantized version, but we use `model_quantized`."
      ],
      "metadata": {
        "id": "ZmCfqkN8jl7i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohi4NSJTnm4f"
      },
      "outputs": [],
      "source": [
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")  # instruction\n",
        "\n",
        "# Tokenize inputs\n",
        "inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(model.device)  # Move inputs to model's device\n",
        "\n",
        "# Initialize text streamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Generate response with streamer\n",
        "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Using `TextIteratorStreamer` and Metrics with Quantized Model\n",
        "\n",
        "Finally, we repeat the performance metric calculation using `TextIteratorStreamer` with the 4-bit quantized model. This will help compare its TTFT, ITL, latency, and throughput against the non-quantized version."
      ],
      "metadata": {
        "id": "T2DGt0XJj1iG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaRLZQ8rnm4f"
      },
      "outputs": [],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "# Define Alpaca-style prompt format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare input text\n",
        "prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")\n",
        "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Initialize variables for time measurements\n",
        "start_time = time.time()\n",
        "token_times = []\n",
        "\n",
        "# Initialize TextIteratorStreamer\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
        "\n",
        "# Start generation in a separate thread\n",
        "thread = Thread(target=model.generate, kwargs={\n",
        "    'input_ids': inputs['input_ids'],\n",
        "    'attention_mask': inputs['attention_mask'],\n",
        "    'streamer': streamer,\n",
        "    'max_new_tokens': 100\n",
        "})\n",
        "thread.start()\n",
        "\n",
        "# Initialize a variable to store the model output\n",
        "model_output = \"\"\n",
        "first_token_time = None\n",
        "\n",
        "# Iterate over the streamer to get the generated text in chunks\n",
        "for i, new_text in enumerate(streamer):\n",
        "    model_output += new_text\n",
        "    print(new_text, end='')\n",
        "\n",
        "    # Measure time for the first token\n",
        "    if i == 0:\n",
        "        first_token_time = time.time()\n",
        "    # Measure time for each token\n",
        "    token_times.append(time.time())\n",
        "\n",
        "# Calculate end-to-end latency\n",
        "end_time = time.time()\n",
        "end_to_end_latency = end_time - start_time\n",
        "\n",
        "# Calculate Time To First Token (TTFT)\n",
        "ttft = first_token_time - start_time if first_token_time else 0\n",
        "\n",
        "# Calculate Inter-Token Latency (ITL)\n",
        "itl = sum(x - y for x, y in zip(token_times[1:], token_times[:-1])) / (len(token_times) - 1) if len(token_times) > 1 else 0\n",
        "\n",
        "# Calculate throughput\n",
        "throughput = len(tokenizer.encode(model_output)) / end_to_end_latency if model_output else 0\n",
        "\n",
        "print(\"\\nTime To First Token (TTFT):\", ttft)\n",
        "print(\"Inter-token latency (ITL):\", itl)\n",
        "print(\"End-to-end Latency:\", end_to_end_latency)\n",
        "print(\"Throughput:\", throughput)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Conclusion\n",
        "\n",
        "This notebook demonstrated text streaming with `TextStreamer` and `TextIteratorStreamer` using both a full-precision and a 4-bit quantized version of the `unsloth/Meta-Llama-3.1-8B-Instruct` model.\n",
        "\n",
        "Key takeaways:\n",
        "- **TextStreamer** is straightforward for displaying streamed output directly.\n",
        "- **TextIteratorStreamer** offers more control for programmatic handling of streamed tokens and is suitable for calculating performance metrics.\n",
        "- **Quantization** significantly reduces GPU memory usage, which can be observed using the provided utility functions. This often comes with a trade-off in inference speed (TTFT, ITL, throughput) and potentially a minor impact on output quality, which can be compared by examining the metrics from both sections.\n",
        "\n",
        "By comparing the performance metrics and memory usage, users can make informed decisions about whether to use quantization for their specific application based on available hardware and performance requirements."
      ],
      "metadata": {
        "id": "PsxIdRtNkTc6"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}