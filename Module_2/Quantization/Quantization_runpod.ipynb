{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization model \"facebook/opt-350m\" with load_in_4bit, float16, quant_type of \"nf4\" with BitsAndBytes, pushed model to HuggingFace and deployed Endpoint in runpod."
      ],
      "metadata": {
        "id": "kQpzgDJiV0-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from typing import Generator, Dict, Any, List\n",
        "\n",
        "# RunPod API configuration\n",
        "RUNPOD_API_URL = \"https://api.runpod.ai/v2/eqm3i8nntmabro/run\"\n",
        "RUNPOD_STREAM_URL = \"https://api.runpod.ai/v2/eqm3i8nntmabro/stream\"\n",
        "API_KEY = \"\"  # Replace with your actual API key\n",
        "\n",
        "# Prepare the prompt using an Alpaca-style format\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "class RunPodStreamer:\n",
        "    \"\"\"Custom streamer class for RunPod API.\"\"\"\n",
        "\n",
        "    def __init__(self, job_id: str, api_key: str):\n",
        "        self.job_id = job_id\n",
        "        self.api_key = api_key\n",
        "        self.headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': f'Bearer {api_key}'\n",
        "        }\n",
        "        self.url = f\"{RUNPOD_STREAM_URL}/{job_id}\"\n",
        "        self._stop = False\n",
        "        self._seen_tokens = set()\n",
        "\n",
        "    def _extract_tokens(self, data: Dict) -> List[str]:\n",
        "        \"\"\"Extract tokens from the RunPod response structure.\"\"\"\n",
        "        tokens = []\n",
        "\n",
        "        try:\n",
        "            # Navigate through the nested structure\n",
        "            if 'stream' in data and data['stream']:\n",
        "                for stream_item in data['stream']:\n",
        "                    if 'output' in stream_item:\n",
        "                        output = stream_item['output']\n",
        "                        if 'choices' in output:\n",
        "                            for choice in output['choices']:\n",
        "                                if 'tokens' in choice:\n",
        "                                    tokens.extend(choice['tokens'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting tokens: {e}\")\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def __iter__(self) -> Generator[str, None, None]:\n",
        "        \"\"\"Iterator that yields text chunks as they arrive from the RunPod stream.\"\"\"\n",
        "        all_tokens = []\n",
        "        last_yielded_index = 0\n",
        "\n",
        "        while not self._stop:\n",
        "            try:\n",
        "                response = requests.get(self.url, headers=self.headers)\n",
        "                response.raise_for_status()\n",
        "                result = response.json()\n",
        "\n",
        "                status = result.get('status')\n",
        "\n",
        "                if status == 'IN_PROGRESS':\n",
        "                    # Extract tokens from the current response\n",
        "                    new_tokens = self._extract_tokens(result)\n",
        "                    if new_tokens:\n",
        "                        # Only yield new tokens that haven't been yielded before\n",
        "                        if len(new_tokens) > len(all_tokens):\n",
        "                            # Yield only the new tokens\n",
        "                            for token in new_tokens[len(all_tokens):]:\n",
        "                                yield token\n",
        "                                all_tokens.append(token)\n",
        "\n",
        "                elif status == 'COMPLETED':\n",
        "                    self._stop = True\n",
        "                    break\n",
        "\n",
        "                elif status == 'FAILED':\n",
        "                    raise Exception(f\"Job failed: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "                time.sleep(0.1)  # Small delay between polls\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Streaming error: {e}\")\n",
        "                self._stop = True\n",
        "                break\n",
        "\n",
        "def send_runpod_request(prompt: str) -> str:\n",
        "    \"\"\"Sends a request to the RunPod API endpoint and returns the job ID.\"\"\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {API_KEY}'\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"input\": {\n",
        "            \"prompt\": prompt,\n",
        "            \"max_new_tokens\": 100,\n",
        "            # Add other parameters as needed by your RunPod endpoint\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    response = requests.post(RUNPOD_API_URL, headers=headers, json=data)\n",
        "    response.raise_for_status()\n",
        "    result = response.json()\n",
        "    return result['id']\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Prepare input text\n",
        "    prompt_text = alpaca_prompt.format(\"What is the importance of using renewable energy?\")\n",
        "\n",
        "    # Initialize variables for time measurements\n",
        "    start_time = time.time()\n",
        "    token_times = []\n",
        "    first_token_time = None\n",
        "    model_output = \"\"\n",
        "\n",
        "    try:\n",
        "        # Send the request to RunPod\n",
        "        print(\"Sending request to RunPod...\")\n",
        "        job_id = send_runpod_request(prompt_text)\n",
        "        print(f\"Job ID: {job_id}\")\n",
        "\n",
        "        # Create streamer\n",
        "        streamer = RunPodStreamer(job_id, API_KEY)\n",
        "\n",
        "        # Start streaming in main thread\n",
        "        print(\"\\nResponse:\")\n",
        "        for i, new_text in enumerate(streamer):\n",
        "            model_output += new_text\n",
        "            print(new_text, end='', flush=True)\n",
        "\n",
        "            # Measure time for the first token\n",
        "            if i == 0:\n",
        "                first_token_time = time.time()\n",
        "\n",
        "            # Measure time for each token/chunk\n",
        "            token_times.append(time.time())\n",
        "\n",
        "        # Calculate end-to-end latency\n",
        "        end_time = time.time()\n",
        "        end_to_end_latency = end_time - start_time\n",
        "\n",
        "        # Calculate Time To First Token (TTFT)\n",
        "        ttft = first_token_time - start_time if first_token_time else 0\n",
        "\n",
        "        # Calculate Inter-Token Latency (ITL)\n",
        "        itl = sum(x - y for x, y in zip(token_times[1:], token_times[:-1])) / (len(token_times) - 1) if len(token_times) > 1 else 0\n",
        "\n",
        "        # Calculate Throughput\n",
        "        # Count tokens - RunPod returns them as a list\n",
        "        token_count = len(model_output.split()) if ' ' in model_output else len([t for t in model_output.split('\\n') if t])\n",
        "        throughput = token_count / end_to_end_latency if model_output else 0\n",
        "\n",
        "        print(\"\\n\\nPerformance Metrics:\")\n",
        "        print(f\"Time To First Token (TTFT): {ttft:.3f} seconds\")\n",
        "        print(f\"Inter-token latency (ITL): {itl:.3f} seconds\")\n",
        "        print(f\"End-to-end Latency: {end_to_end_latency:.3f} seconds\")\n",
        "        print(f\"Throughput: {throughput:.3f} tokens/second\")\n",
        "        print(f\"Total tokens: {token_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx4szc58WZND",
        "outputId": "4c9359e1-9ca1-47af-bb18-715f94523164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending request to RunPod...\n",
            "Job ID: b17fddbe-18e8-4dc4-8af2-6270c7f4460e-u1\n",
            "\n",
            "Response:\n",
            "What is accurate?\n",
            "What can be reitled? What can be delegated? Do we need to have a single response?\n",
            "What potential problems might arise? Why should I keep sending messages?\n",
            "From Master Total Array subscribers\n",
            "\n",
            "### Instruction:\n",
            "minimal text messages is easy, straightforward methods\n",
            "comparing simple and valid content\n",
            "If I send a relevant a message and receive a response, I do not need to read/print the whole message. Note: If I send and receive in\n",
            "\n",
            "Performance Metrics:\n",
            "Time To First Token (TTFT): 1.393 seconds\n",
            "Inter-token latency (ITL): 0.000 seconds\n",
            "End-to-end Latency: 1.597 seconds\n",
            "Throughput: 47.598 tokens/second\n",
            "Total tokens: 76\n"
          ]
        }
      ]
    }
  ]
}
